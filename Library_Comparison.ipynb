{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Machine Learning Libraries\n",
    "\n",
    "For this overview example, we will create a classification model using:\n",
    "\n",
    "1. scikit-learn\n",
    "2. Keras\n",
    "3. PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X_scaled = StandardScaler().fit_transform(cancer.data)\n",
    "print(\"Original data (rows, features):\", X_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generating to polynomial features is not that time consuming\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X_scaled)\n",
    "print(\"All polynomial features (order 2):\", X_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# A fairly generic random forest\n",
    "rfc = RandomForestClassifier(max_depth=7, n_estimators=10, random_state=1)\n",
    "\n",
    "# Do some work to pick the optimal number of features\n",
    "# \"Recursive feature elimination using cross-validation\"\n",
    "rfecv = RFECV(estimator=rfc, cv=5, n_jobs=-1)\n",
    "X_poly_top = rfecv.fit_transform(X_poly, cancer.target)\n",
    "\n",
    "# The \"top\" features selected for the model\n",
    "print(\"Best polynomial features\", X_poly_top.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Do a train/test split on the \"poly_top\" features\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_poly_top, cancer.target, random_state=42)\n",
    "\n",
    "# Train the selected RFC model\n",
    "rfc = RandomForestClassifier(max_depth=7, n_estimators=10, random_state=1)\n",
    "print(\"Test accuracy:\", rfc.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "There are several things to notice in our NN setup.  We do *not* generate polynomial features.  Instead, we allow the network itself to derive them on a first layer we arrange to have the same number of neurons as there were polynomial features in our Random Forest approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "in_dim = cancer.data.shape[1]\n",
    "hidden1 = X_poly_top.shape[1]   # The size of layer that deduces poly features\n",
    "hidden2 = 20                    # The size of the \"inference layer\"\n",
    "out_dim = 1                     # Output a single value\n",
    "\n",
    "batches_in_data = X_train.shape[0]/batch_size\n",
    "epochs = int(5000/batches_in_data)\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Split the original data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                           cancer.data, cancer.target, random_state=42)\n",
    "cancer.data.shape   # The shape of the data being split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k = keras.models.Sequential([\n",
    "    # This layer allows \"polynomial features\"\n",
    "    keras.layers.Dense(hidden1, activation='relu', input_shape=(in_dim,)),\n",
    "    # This layer is the essential \"inference\"\n",
    "    keras.layers.Dense(hidden2),\n",
    "    # Often Leaky ReLU eliminates the \"dead neuron\" danger\n",
    "    keras.layers.LeakyReLU(),\n",
    "    # A Dropout layer sometimes reduces co-adaptation of neurons\n",
    "    keras.layers.Dropout(rate=0.25),\n",
    "    # A sigmoid activation is used for a binary decision\n",
    "    keras.layers.Dense(out_dim, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# The default optimization is Root Mean Square Propogation\n",
    "model_k.compile(loss='mean_squared_error',\n",
    "                optimizer=RMSprop(lr=learning_rate),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model_k.fit(X_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      verbose=False,\n",
    "                      validation_data=(X_test, y_test))\n",
    "\n",
    "score = model_k.evaluate(X_test, y_test, verbose=True)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sometimes we do better using Adaptive Moment Optimization\n",
    "model_k.compile(loss='mean_squared_error',\n",
    "                optimizer=Adam(lr=learning_rate),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model_k.fit(X_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      verbose=False,\n",
    "                      validation_data=(X_test, y_test))\n",
    "score = model_k.evaluate(X_test, y_test, verbose=True)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential NN\n",
    "model_t = torch.nn.Sequential(\n",
    "    # This layer allows \"polynomial features\"\n",
    "    torch.nn.Linear(in_dim, hidden1),\n",
    "    # The activation is treated as a separate layer\n",
    "    torch.nn.ReLU(),\n",
    "    # This layer is the essential \"inference\"\n",
    "    torch.nn.Linear(hidden1, hidden2),\n",
    "    # Often Leaky ReLU eliminates the \"dead neuron\" danger\n",
    "    torch.nn.LeakyReLU(), \n",
    "    # A Dropout layer sometimes reduces co-adaptation of neurons\n",
    "    torch.nn.Dropout(p=0.25),\n",
    "    # A sigmoid activation is used for a binary decision\n",
    "    torch.nn.Linear(hidden2, out_dim),  \n",
    "    torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import device, cuda\n",
    "from torchsummary import summary\n",
    "\n",
    "# torchsummary has a glitch. If running on a CUDA-enabled build\n",
    "# it only wants to print a CUDA model\n",
    "if cuda.is_available():\n",
    "    model_t = model_t.to(device('cuda'))\n",
    "    \n",
    "summary(model_t, input_size=(1,in_dim))\n",
    "\n",
    "model_t = model_t.to(device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_every = 250\n",
    "\n",
    "def do_training():\n",
    "    for t in range(5000):\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = model_t(X)\n",
    "\n",
    "        # Compute and print loss.\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        if not t % show_every:\n",
    "            y_test_pred = model_t(Variable(X_test_T))\n",
    "            prediction = [int(x > 0.5) for x in y_test_pred.data.numpy()]\n",
    "            test_accuracy = (prediction == y_test).sum() / len(y_test)\n",
    "            train_pred = [int(x > 0.5) for x in y_pred.data.numpy()]\n",
    "            train_accuracy = (train_pred == y_train).sum() / len(y_train)\n",
    "            print(\"Batch: %04d | Training Loss: %6.2f | Train accuracy: %.4f | Test accuracy: %.4f\" % (\n",
    "                          t, loss.item(), train_accuracy, test_accuracy))\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). This is because by default, gradients are\n",
    "        # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Now run model\n",
    "X = torch.from_numpy(X_train).float()\n",
    "y = torch.from_numpy(y_train[:, np.newaxis]).float()\n",
    "X_test_T = torch.from_numpy(X_test).float()\n",
    "y_test_T = torch.from_numpy(y_test[:, np.newaxis]).float()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.RMSprop(model_t.parameters(), lr=learning_rate)\n",
    "do_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_t.parameters(), lr=learning_rate)\n",
    "do_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a few predictions with trained model\n",
    "\n",
    "Run the below code several times.  Because it uses a Dropout layer, the activated neurons—and hence the exact predictions—will vary on each call.  Ideally the results will be consistent in identifying the binary class, but they will not be precisely identical in floating point value output in range `[0,1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_t(X_test_T[:10])\n",
    "for row, prediction in enumerate(predictions):\n",
    "    print(\"Observation %d; probability benign: %0.3f%%\" % (row, prediction*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from torchvision.transforms import Resize, ToTensor, Compose\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "inception = models.inception_v3(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imagenet labels for 1000 pre-trained image classes\n",
    "class_defs = json.load(open(\"data/imagenet_class_index.json\"))\n",
    "labels = {int(k):name for k, (code, name) in class_defs.items()}\n",
    "\n",
    "# Small utility to load, resize and tensorize images\n",
    "def load_images(fnames):\n",
    "    for fname in fnames:\n",
    "        image = Image.open(fname)\n",
    "        image_t = Compose([Resize(299), ToTensor()])(image).float()\n",
    "        image_t = torch.tensor(image_t, requires_grad=True)\n",
    "        yield image, image_t.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, image_tensor in load_images([\n",
    "            'img/cannot-brain.jpg', \n",
    "            'img/rainbox-butterfly-unicorn-kitten.jpg',\n",
    "            'img/Crisopid_July_2013-9.jpg',\n",
    "            'img/dqm-bokeh-palms.jpg']):\n",
    "    outputs = inception(image_tensor)\n",
    "    prediction = np.argmax(outputs.detach().numpy())\n",
    "    display(image)\n",
    "    print(labels[prediction])\n",
    "    print('—'*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "\n",
    "**Diving Deeper**: We have seen a few brief examples of PyTorch in use, and illustrated a little bit about how its APIs differ from those of other libraries.  Next we will look at some of the essential concept in the design of PyTorch.\n",
    "\n",
    "<a href=\"IntroPyTorch.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
